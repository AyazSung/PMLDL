{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:32:18.824642100Z",
     "start_time": "2023-11-05T16:32:06.513075800Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           reference  \\\n0  alkar flood psychic waste explain high level n...   \n1                                          get nasty   \n2                          well could spare life one   \n3                                 ah monkey get snap   \n4                                      get order put   \n\n                                         translation  similarity  lenght_diff  \\\n0  alkar flood mental waste would explain high le...    0.785171     0.010309   \n1                                     become disgust    0.749687     0.071429   \n2                                    well spare life    0.919051     0.268293   \n3                                        monkey wake    0.664333     0.309524   \n4                                         order kill    0.726639     0.181818   \n\n    ref_tox   trn_tox  ref_len  trn_len  \n0  0.014195  0.981983        8        9  \n1  0.065473  0.999039        2        2  \n2  0.213313  0.985068        5        3  \n3  0.053362  0.994215        4        2  \n4  0.009402  0.999348        3        2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>similarity</th>\n      <th>lenght_diff</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n      <th>ref_len</th>\n      <th>trn_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>alkar flood psychic waste explain high level n...</td>\n      <td>alkar flood mental waste would explain high le...</td>\n      <td>0.785171</td>\n      <td>0.010309</td>\n      <td>0.014195</td>\n      <td>0.981983</td>\n      <td>8</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>get nasty</td>\n      <td>become disgust</td>\n      <td>0.749687</td>\n      <td>0.071429</td>\n      <td>0.065473</td>\n      <td>0.999039</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>well could spare life one</td>\n      <td>well spare life</td>\n      <td>0.919051</td>\n      <td>0.268293</td>\n      <td>0.213313</td>\n      <td>0.985068</td>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ah monkey get snap</td>\n      <td>monkey wake</td>\n      <td>0.664333</td>\n      <td>0.309524</td>\n      <td>0.053362</td>\n      <td>0.994215</td>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>get order put</td>\n      <td>order kill</td>\n      <td>0.726639</td>\n      <td>0.181818</td>\n      <td>0.009402</td>\n      <td>0.999348</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read tsv file cleaned_new\n",
    "df = pd.read_csv('../data/final_cleaned.tsv', sep='\\t')\n",
    "df = df[:1000]\n",
    "df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:32:20.188806400Z",
     "start_time": "2023-11-05T16:32:18.824642100Z"
    }
   },
   "id": "d276ed93c98160e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embeddings:\n",
    "\n",
    "Convert your tokenized text into numerical representations using word embeddings. You can use pre-trained embeddings (Word2Vec, GloVe, BERT) to capture the semantic meaning of words.\n",
    "\n",
    "Padding and Sequence Length:\n",
    "Ensure that all input sequences (toxic text) and output sequences (non-toxic text) have the same length. You may need to pad shorter sequences with a special token (e.g., <PAD>) or truncate longer sequences to a fixed length."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c84023fe3bdd20f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:32:24.156881Z",
     "start_time": "2023-11-05T16:32:20.171756700Z"
    }
   },
   "id": "bc923701e1f4d4b5"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "texts = df['reference'].tolist() + df['translation'].tolist()\n",
    "toxicity = df['ref_tox'].tolist() + df['trn_tox'].tolist()\n",
    "toxicity = [1 if x > 0.35 else 0 for x in toxicity]\n",
    "input_ids = []\n",
    "attention_masks = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:32:24.186446300Z",
     "start_time": "2023-11-05T16:32:24.139810900Z"
    }
   },
   "id": "c5db3a770080e900"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\SunagatullinAyaz\\.conda\\envs\\pythonProject2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for text in texts:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=64,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:32:25.074231900Z",
     "start_time": "2023-11-05T16:32:24.154859700Z"
    }
   },
   "id": "67761c479f2b6d5a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "toxicity = torch.tensor(toxicity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:32:25.133014100Z",
     "start_time": "2023-11-05T16:32:25.078363Z"
    }
   },
   "id": "43b3ff5946fcd1af"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attention_masks, toxicity)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:32:25.133014100Z",
     "start_time": "2023-11-05T16:32:25.115007200Z"
    }
   },
   "id": "9f285fd7516ba0b0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:32:25.157489200Z",
     "start_time": "2023-11-05T16:32:25.124540200Z"
    }
   },
   "id": "8637d88450d5df26"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:32:34.853525200Z",
     "start_time": "2023-11-05T16:32:25.157489200Z"
    }
   },
   "id": "2ca15a35e358e12c"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Average train loss: 0.6990800607204437\n",
      "Epoch: 2\n",
      "Average train loss: 0.6645559120178223\n",
      "Epoch: 3\n",
      "Average train loss: 0.5889290785789489\n",
      "Epoch: 4\n",
      "Average train loss: 0.49561797618865966\n",
      "Epoch: 5\n",
      "Average train loss: 0.39916676640510557\n",
      "Epoch: 6\n",
      "Average train loss: 0.31641487032175064\n",
      "Epoch: 7\n",
      "Average train loss: 0.24131288081407548\n",
      "Epoch: 8\n",
      "Average train loss: 0.21309284888207913\n",
      "Epoch: 9\n",
      "Average train loss: 0.15780247017741203\n",
      "Epoch: 10\n",
      "Average train loss: 0.14695286829024554\n",
      "Epoch: 11\n",
      "Average train loss: 0.1122309335693717\n",
      "Epoch: 12\n",
      "Average train loss: 0.10008319124579429\n",
      "Epoch: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch:', epoch + 1)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dl:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_loss / len(train_dl)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:40:17.926544900Z",
     "start_time": "2023-11-05T16:32:34.862046200Z"
    }
   },
   "id": "47a1450ce807b29b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dl:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "        labels_dlat = label_ids.flatten()\n",
    "\n",
    "        total += labels_dlat.size\n",
    "        correct += np.sum(pred_flat == labels_dlat)\n",
    "\n",
    "print('Accuracy: %d %%' % (100 * correct / total))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:40:17.942596800Z",
     "start_time": "2023-11-05T16:40:17.930544200Z"
    }
   },
   "id": "8fec536f477f4f79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_toxicity(text, model):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=64,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    return pred_flat[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:40:17.934072Z"
    }
   },
   "id": "7c1f64f68b7f008b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(predict_toxicity('I love you'))\n",
    "print(predict_toxicity('I hate you'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:40:17.938098700Z"
    }
   },
   "id": "918ae41e5aebd046"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(predict_toxicity('I hate you so much asshole'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T16:40:18.002673400Z",
     "start_time": "2023-11-05T16:40:17.942596800Z"
    }
   },
   "id": "37ae1aab27f319de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(predict_toxicity('I love you so much'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:40:17.947143900Z"
    }
   },
   "id": "45531543d87ebb2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(model.state_dict(), '../models/classification_model_NEW.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:40:17.951138700Z"
    }
   },
   "id": "fd0df87089ae117e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load saved model\n",
    "model_my = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:40:17.954659900Z"
    }
   },
   "id": "77e937cce33f518a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_my.load_state_dict(torch.load('../models/classification_model_NEW.pt'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:40:17.959657800Z"
    }
   },
   "id": "ceb217c463582d95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_toxicity(text, model):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=64,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    outputs = model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    return pred_flat[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:40:17.961658500Z"
    }
   },
   "id": "4b03b1b4aafc1be3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_my.to(device)\n",
    "predict_toxicity('I love you', model_my)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:40:17.963170500Z"
    }
   },
   "id": "c7f3b523c8ac156d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-05T16:40:17.965185Z"
    }
   },
   "id": "3d6e5f56d7513870"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
